# Hyperparameter Search - NEN-V Optimization System

Sistema completo de otimizaÃ§Ã£o automÃ¡tica de hiperparÃ¢metros para a rede neural NEN-V.

## ğŸ¯ VisÃ£o Geral

O sistema de busca de hiperparÃ¢metros permite encontrar automaticamente a melhor configuraÃ§Ã£o para a rede neural, otimizando **45+ parÃ¢metros** distribuÃ­dos em 12 categorias diferentes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYPERPARAMETER OPTIMIZATION SYSTEM                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Parameter      â”‚    â”‚    Search       â”‚    â”‚   Evaluation    â”‚        â”‚
â”‚  â”‚  Space          â”‚â”€â”€â”€â–¶â”‚    Strategy     â”‚â”€â”€â”€â–¶â”‚   System        â”‚        â”‚
â”‚  â”‚  (45+ params)   â”‚    â”‚  (Bayesian/etc) â”‚    â”‚  (Benchmarks)   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                      â”‚                      â”‚                  â”‚
â”‚           â–¼                      â–¼                      â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚                    Experiment Orchestrator                       â”‚      â”‚
â”‚  â”‚  â€¢ Parallel execution        â€¢ Early stopping                   â”‚      â”‚
â”‚  â”‚  â€¢ Result logging            â€¢ Best config tracking             â”‚      â”‚
â”‚  â”‚  â€¢ Checkpointing             â€¢ Progress visualization           â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

```bash
# Teste rÃ¡pido (10 trials)
cargo run --release --bin hyperopt -- --quick

# Bayesian Optimization (recomendado)
cargo run --release --bin hyperopt -- --strategy bayesian --trials 100

# Ver todas as opÃ§Ãµes
cargo run --release --bin hyperopt -- --help
```

## ğŸ“ Estrutura de Arquivos

```
hyperparameter_search/
â”œâ”€â”€ main.rs           # CLI principal e entry point
â”œâ”€â”€ mod.rs            # MÃ³dulo e re-exports
â”œâ”€â”€ param_space.rs    # DefiniÃ§Ã£o do espaÃ§o de parÃ¢metros (45+)
â”œâ”€â”€ search.rs         # Algoritmos de busca (4 estratÃ©gias)
â”œâ”€â”€ evaluation.rs     # Sistema de benchmarks e mÃ©tricas
â”œâ”€â”€ orchestrator.rs   # CoordenaÃ§Ã£o de experimentos
â””â”€â”€ README.md         # Este arquivo
```

## ğŸ”§ Uso Detalhado

### OpÃ§Ãµes da CLI

```
USAGE:
    cargo run --release --bin hyperopt -- [OPTIONS]

OPTIONS:
    --strategy <NAME>     EstratÃ©gia de busca: bayesian, random, evolutionary
                         (default: bayesian)

    --trials <N>         NÃºmero mÃ¡ximo de trials (default: 100)

    --population <N>     Tamanho da populaÃ§Ã£o para evolutionary (default: 20)

    --importance <F>     ImportÃ¢ncia mÃ­nima de parÃ¢metros [0.0-1.0]
                         (default: 0.6)

    --output <DIR>       DiretÃ³rio de saÃ­da para resultados
                         (default: experiments/results)

    --seed <N>           Seed para reprodutibilidade (default: 42)

    --patience <N>       Early stopping patience (default: 20)

    --name <NAME>        Nome do experimento (default: hyperopt)

    --quick              Teste rÃ¡pido com 10 trials

    --quiet              Suprime output verboso

    --help               Mostra esta mensagem de ajuda
```

### Exemplos de Uso

```bash
# OtimizaÃ§Ã£o completa com Bayesian (melhor qualidade)
cargo run --release --bin hyperopt -- \
    --strategy bayesian \
    --trials 200 \
    --name meu_experimento

# Busca rÃ¡pida com Random Search
cargo run --release --bin hyperopt -- \
    --strategy random \
    --trials 50 \
    --patience 10

# Evolutionary com populaÃ§Ã£o grande
cargo run --release --bin hyperopt -- \
    --strategy evolutionary \
    --trials 500 \
    --population 50

# Otimizar apenas parÃ¢metros mais importantes
cargo run --release --bin hyperopt -- \
    --importance 0.8 \
    --trials 100

# Experimento reprodutÃ­vel
cargo run --release --bin hyperopt -- \
    --seed 12345 \
    --trials 100
```

## ğŸ“Š EstratÃ©gias de Busca

### 1. Bayesian Optimization (Recomendado)

```rust
// Usa Gaussian Process como modelo surrogate
// Upper Confidence Bound (UCB) como funÃ§Ã£o de aquisiÃ§Ã£o
BayesianSearch::new(seed)
    .with_exploration(2.0)  // kappa para UCB
```

**Vantagens:**
- Mais eficiente em nÃºmero de trials
- Aprende com resultados anteriores
- Bom balance exploraÃ§Ã£o/exploitation

**Quando usar:**
- Poucos trials disponÃ­veis (<200)
- ParÃ¢metros contÃ­nuos
- FunÃ§Ã£o objetivo suave

### 2. Random Search

```rust
// Amostragem uniforme do espaÃ§o de parÃ¢metros
RandomSearch::new(seed)
```

**Vantagens:**
- Simples e rÃ¡pido
- ParalelizÃ¡vel
- Bom baseline

**Quando usar:**
- Baseline inicial
- Muitos trials disponÃ­veis
- EspaÃ§o de alta dimensÃ£o

### 3. Evolutionary Search

```rust
// Algoritmo genÃ©tico com seleÃ§Ã£o, crossover e mutaÃ§Ã£o
EvolutionarySearch::new(seed, population_size)
    .with_mutation_rate(0.1)
    .with_crossover_rate(0.8)
```

**Vantagens:**
- Bom para espaÃ§os discretos
- MantÃ©m diversidade
- Pode escapar de mÃ­nimos locais

**Quando usar:**
- ParÃ¢metros categÃ³ricos/discretos
- Muitos trials disponÃ­veis
- Landscape multimodal

### 4. Grid Search

```rust
// Busca exaustiva em grade
GridSearch::new(points_per_param)
```

**Vantagens:**
- Cobertura completa
- DeterminÃ­stico
- FÃ¡cil de entender

**Quando usar:**
- Poucos parÃ¢metros (<5)
- NecessÃ¡rio cobertura completa
- Debugging

## ğŸ“ˆ EspaÃ§o de ParÃ¢metros

### Categorias e ParÃ¢metros

| Categoria | # Params | ParÃ¢metros Principais |
|-----------|----------|----------------------|
| **timing** | 6 | `stdp_window`, `stdp_tau_plus`, `stdp_tau_minus`, `eligibility_trace_tau`, `refractory_period`, `stp_recovery_tau` |
| **learning** | 7 | `base_learning_rate`, `stdp_a_plus`, `stdp_a_minus`, `ltp_ltd_ratio`, `weight_decay`, `trace_increment`, `istdp_rate` |
| **homeostasis** | 6 | `target_firing_rate`, `homeo_eta`, `homeo_interval`, `memory_alpha`, `meta_threshold`, `meta_alpha` |
| **energy** | 4 | `max_energy`, `cost_fire_ratio`, `recovery_rate`, `plasticity_cost_factor` |
| **memory** | 5 | `weight_clamp`, `tag_decay_rate`, `capture_threshold`, `dopamine_sensitivity`, `consolidation_rate` |
| **curiosity** | 3 | `scale`, `surprise_threshold`, `habituation_rate` |
| **network** | 4 | `inhibitory_ratio`, `initial_threshold`, `initial_exc_weight`, `initial_inh_weight` |
| **working_memory** | 3 | `capacity`, `recurrent_strength`, `decay_rate` |
| **predictive** | 2 | `state_learning_rate`, `inference_iterations` |
| **competition** | 2 | `strength`, `interval` |
| **sleep** | 2 | `interval`, `replay_noise` |
| **stp** | 1 | `use_fraction` |

### NÃ­veis de ImportÃ¢ncia

Os parÃ¢metros tÃªm scores de importÃ¢ncia de 0.0 a 1.0:

```
ImportÃ¢ncia 0.9+ (CrÃ­ticos):
  â€¢ learning.base_learning_rate
  â€¢ learning.stdp_a_plus
  â€¢ homeostasis.target_firing_rate
  â€¢ timing.stdp_window

ImportÃ¢ncia 0.7-0.9 (Importantes):
  â€¢ timing.eligibility_trace_tau
  â€¢ network.inhibitory_ratio
  â€¢ memory.weight_clamp
  â€¢ curiosity.scale

ImportÃ¢ncia 0.5-0.7 (Moderados):
  â€¢ working_memory.capacity
  â€¢ predictive.state_learning_rate
  â€¢ competition.strength

ImportÃ¢ncia <0.5 (SecundÃ¡rios):
  â€¢ sleep.replay_noise
  â€¢ stp.use_fraction
```

Use `--importance` para filtrar parÃ¢metros por importÃ¢ncia mÃ­nima.

## ğŸ¯ Sistema de AvaliaÃ§Ã£o

### Benchmarks DisponÃ­veis

#### TaskBenchmark
Avalia performance em tarefas de navegaÃ§Ã£o/RL.
```rust
TaskBenchmark::navigation(episodes: 50, max_steps: 500)
```
- Reward total obtido
- Taxa de sucesso
- Steps mÃ©dios por episÃ³dio

#### ConvergenceBenchmark
Mede velocidade de convergÃªncia do aprendizado.
```rust
ConvergenceBenchmark::new(max_steps: 10000, threshold: 0.01)
```
- Steps atÃ© convergÃªncia
- Estabilidade final

#### StabilityBenchmark
Avalia consistÃªncia atravÃ©s de mÃºltiplas execuÃ§Ãµes.
```rust
StabilityBenchmark::new(num_runs: 5, steps_per_run: 1000)
```
- Coeficiente de variaÃ§Ã£o
- Desvio padrÃ£o do reward

#### EfficiencyBenchmark
Mede eficiÃªncia energÃ©tica e computacional.
```rust
EfficiencyBenchmark::new(num_steps: 1000)
```
- Reward por unidade de energia
- Taxa de disparo vs performance

### Pesos dos Benchmarks

```rust
MetricWeights {
    reward: 0.4,      // Performance na tarefa
    success: 0.3,     // Taxa de sucesso
    convergence: 0.1, // Velocidade de aprendizado
    stability: 0.1,   // ConsistÃªncia
    efficiency: 0.1,  // EficiÃªncia energÃ©tica
}
```

## ğŸ“‚ Output e Resultados

### Arquivos Gerados

```
experiments/results/
â”œâ”€â”€ <name>_log.csv           # Log trial-by-trial
â”œâ”€â”€ <name>_results.txt       # Resumo final
â””â”€â”€ <name>_checkpoint.json   # Checkpoint periÃ³dico
```

### Formato do Log CSV

```csv
trial,score,duration_ms,status,config
0,0.5665,113,Completed,"learning.base_learning_rate=0.01;..."
1,0.6439,44,Completed,"learning.base_learning_rate=0.02;..."
```

### Exemplo de Resultado Final

```
=== HYPERPARAMETER OPTIMIZATION RESULTS ===
Experiment: meu_experimento
Strategy: BayesianOptimization
Trials: 100
Best Score: 0.723456

=== BEST CONFIGURATION ===
learning.base_learning_rate: Float(0.0156)
homeostasis.target_firing_rate: Float(0.1234)
timing.stdp_window: Int(45)
...

=== TOP 10 TRIALS ===
1. Trial 87 - Score: 0.723456
2. Trial 92 - Score: 0.718234
3. Trial 76 - Score: 0.712891
...
```

## âš¡ Early Stopping

O sistema para automaticamente se nÃ£o houver melhoria:

```rust
ExperimentConfig {
    early_stopping_patience: Some(20),  // Para apÃ³s 20 trials sem melhoria
    min_improvement: 0.001,             // Threshold mÃ­nimo de melhoria
}
```

## ğŸ§ª Testes

```bash
# Todos os testes do mÃ³dulo
cargo test --bin hyperopt

# Testes especÃ­ficos
cargo test --bin hyperopt test_bayesian
cargo test --bin hyperopt test_evolutionary
cargo test --bin hyperopt test_early_stopping
```

## ğŸ”Œ IntegraÃ§Ã£o ProgramÃ¡tica

### Uso como Biblioteca

```rust
use experiments::hyperparameter_search::{
    ExperimentConfig,
    ExperimentOrchestrator,
    OptimizationObjective,
};

fn main() {
    let config = ExperimentConfig {
        name: "custom_experiment".to_string(),
        max_trials: 50,
        early_stopping_patience: Some(10),
        min_param_importance: 0.7,
        verbose: true,
        ..Default::default()
    };

    let mut experiment = ExperimentOrchestrator::with_bayesian(config);
    let result = experiment.run();

    if let Some(best) = result.best_trial {
        println!("Best score: {}", best.score);
        println!("Best config: {:?}", best.config);
    }
}
```

### Criando EstratÃ©gia Custom

```rust
use experiments::hyperparameter_search::search::{SearchStrategy, SearchResult};

struct MyCustomSearch {
    // ...
}

impl SearchStrategy for MyCustomSearch {
    fn suggest(&mut self, space: &ParameterSpace) -> HashMap<String, ParameterValue> {
        // Sua lÃ³gica de sugestÃ£o
    }

    fn register_result(&mut self, result: SearchResult) {
        // Registra resultado para aprendizado
    }

    fn best_result(&self) -> Option<&SearchResult> {
        // Retorna melhor resultado
    }

    fn history(&self) -> &[SearchResult] {
        // Retorna histÃ³rico
    }

    fn name(&self) -> &str {
        "MyCustomSearch"
    }
}
```

## ğŸ“Š Exemplo de Output

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—      â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—   â•‘
â•‘   â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â•‘
â•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â•â•â•â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘  â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•”â•â•â•â•    â•‘
â•‘   â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘       â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘        â•‘
â•‘   â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â•        â•šâ•â•â•â•      â•šâ•â•  â•šâ•â•   â•šâ•â•   â•šâ•â•        â•‘
â•‘                                                                               â•‘
â•‘           HYPERPARAMETER OPTIMIZATION FOR NEURAL NETWORKS                    â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PARAMETER SPACE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ timing         :   6 parameters                             â”‚
â”‚ learning       :   7 parameters                             â”‚
â”‚ homeostasis    :   6 parameters                             â”‚
â”‚ energy         :   4 parameters                             â”‚
â”‚ memory         :   5 parameters                             â”‚
â”‚ curiosity      :   3 parameters                             â”‚
â”‚ network        :   4 parameters                             â”‚
â”‚ working_memory :   3 parameters                             â”‚
â”‚ predictive     :   2 parameters                             â”‚
â”‚ competition    :   2 parameters                             â”‚
â”‚ sleep          :   2 parameters                             â”‚
â”‚ stp            :   1 parameters                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total:  45 parameters                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

>> Using Bayesian Optimization (recommended)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          HYPERPARAMETER OPTIMIZATION EXPERIMENT             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Experiment: hyperopt                                         â•‘
â•‘ Strategy: BayesianOptimization                               â•‘
â•‘ Parameters: 35                                               â•‘
â•‘ Max Trials: 100                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â˜… Trial    0 | Score: +0.5665 | NEW BEST! | 113.6Âµs
  â˜… Trial    1 | Score: +0.6439 | NEW BEST! | 44.6Âµs
    Trial   10 | Score: +0.6201 | Best: +0.6638 | 52.1Âµs
  â˜… Trial   15 | Score: +0.6891 | NEW BEST! | 61.2Âµs
    Trial   20 | Score: +0.6445 | Best: +0.6891 | 48.3Âµs
    ...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    EXPERIMENT COMPLETE                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Total Trials: 100                                            â•‘
â•‘ Best Score: 0.7234                                           â•‘
â•‘ Duration: 1.23s                                              â•‘
â•‘ Reason: MaxTrialsReached                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## ğŸ”— ReferÃªncias

- **Bayesian Optimization**: Snoek, J. et al. (2012). Practical Bayesian Optimization of Machine Learning Algorithms.
- **Random Search**: Bergstra, J. & Bengio, Y. (2012). Random Search for Hyper-Parameter Optimization.
- **Evolutionary Strategies**: Hansen, N. (2006). The CMA Evolution Strategy: A Tutorial.

---

<div align="center">

Parte do projeto **NEN-V v2.0**

</div>
